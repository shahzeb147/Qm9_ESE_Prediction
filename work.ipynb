{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# standard python\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "#import pathlib\n",
    "\n",
    "import os\n",
    "# plotting, especially for jupyter notebooks\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True # breaks for some endpoint labels\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() # needed for tf version 1 or it stages operations but does not do them\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers, losses, metrics\n",
    "from tensorflow.keras import layers, regularizers\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "# local routines\n",
    "#from chemdataprep import load_PDBs,load_countsfromPDB,load_diametersfromPDB,find_chemnames\n",
    "#from toxmathandler import load_tscores\n",
    "\n",
    "#checkpoint_path = \"/home2/ajgreen4/Read-Across_w_GAN/Models/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__,\". Executing eagerly?\",tf.executing_eagerly())\n",
    "print(\"Number of GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weightedviews\n",
    "#import toxmathandler\n",
    "import qm9datapreparation\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qm9datapreparation import all_molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_molecules))\n",
    "print(all_molecules[40000])\n",
    "len(all_molecules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_atoms = set()\n",
    "\n",
    "for molecule in all_molecules:\n",
    "    for atom in molecule:\n",
    "        unique_atoms.add(atom[0])\n",
    "unique_atoms = sorted(unique_atoms)\n",
    "print(\"atom types:\", unique_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightedviews import load_qm9_data\n",
    "from weightedviews import speciesmap\n",
    "ws, vs, Natoms, Nviews = load_qm9_data(all_molecules, speciesmap, setNatoms= None, setNviews= None, carbonbased= False, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese import elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ese_element = np.array(elements)\n",
    "ese_element.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_molecules_train, all_molecules_test, ese_element_train, ese_element_test = train_test_split(all_molecules, ese_element, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_molecules_test))\n",
    "print(len(all_molecules_train))\n",
    "print(len(ese_element_test))\n",
    "print(len(ese_element_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_train = all_molecules_train\n",
    "qm9_test = all_molecules_test\n",
    "qm9_labels_train = ese_element_train\n",
    "qm9_labels_test= ese_element_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_qm9_train, vs_qm9_train, Natoms_train, Nviews_train = load_qm9_data(qm9_train, speciesmap, setNatoms= None, setNviews= 29, carbonbased= False, verbose= 1)\n",
    "qm9G_train = [ws_qm9_train, vs_qm9_train]\n",
    "ws_qm9_test, vs_qm9_test, Natoms_test, Nviews_test = load_qm9_data(qm9_test, speciesmap, setNatoms= None, setNviews= 29, carbonbased= False, verbose= 1)\n",
    "qm9G_test = [ws_qm9_test, vs_qm9_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsG_train = qm9_labels_train\n",
    "labelsG_test = qm9_labels_test\n",
    "Ntoxicity =1\n",
    "ws_train, vs_train = ws_qm9_train, vs_qm9_train\n",
    "ws_test, vs_test = ws_qm9_test, vs_qm9_test\n",
    "dataG_train = [ws_train, vs_train]\n",
    "dataG_test = [ws_test, vs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(labelsG_train))\n",
    "print(type(labelsG_test))\n",
    "print(type(dataG_train))\n",
    "print(type(dataG_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ws_train.shape)\n",
    "print(vs_train.shape)\n",
    "print(ws_test.shape)\n",
    "print(vs_test.shape)\n",
    "print(labelsG_train.shape)\n",
    "print(labelsG_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network code\n",
    "Constructor routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic dense NN\n",
    "def multiDense(Nin,Nout,Nhidden,widthhidden=None,kernel_regularizer=None):\n",
    "    \"\"\"Construct a basic NN with some dense layers.\n",
    "    \n",
    "    :parameter Nin: The number of inputs\n",
    "    :type Nin: int\n",
    "    :parameter Nout: The number of outputs\n",
    "    :type Nout: int\n",
    "    :parameter Nhidden: The number of hidden layers.\n",
    "    :type Nhidden: int\n",
    "    :parameter widthhidden: The width of each hidden layer.\n",
    "        If left at None, Nin + Nout will be used.\n",
    "    :parameter kernel_regularizer: the regularizer to use, such as regularizers.l2(0.001)\n",
    "    :type kernel_regularizer: tensorflow.keras.regularizers.xxx\n",
    "    :returns: The NN model\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    if widthhidden is None:\n",
    "        widthhidden = Nin + Nout\n",
    "    x = inputs = keras.Input(shape=(Nin,), name='multiDense_input')\n",
    "    if kernel_regularizer is not None:\n",
    "        print(\"Using regularization\")\n",
    "    for i in range(Nhidden):\n",
    "        x = layers.Dense(widthhidden, activation='relu', kernel_regularizer=kernel_regularizer,name='dense'+str(i))(x)\n",
    "#        x = layers.Dense(widthhidden, name='dense'+str(i))(x)\n",
    "#        x = tf.nn.leaky_relu(x, alpha=0.05)\n",
    "#    outputs = layers.Dense(Nout, activation='linear',name='multiDense_output')(x)\n",
    "    outputs = layers.Dense(Nout,name='multiDense_output')(x)\n",
    "    #outputs = tf.nn.leaky_relu(outputs, alpha=0.05)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)#, name='multiDense')\n",
    "if 1:\n",
    "    # manual check of multiDense\n",
    "    mmd = multiDense(10,4,3)\n",
    "    mmd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelwrapper(Nparallel,basemodel,insteadmax=False):\n",
    "    \"\"\"Construct a model that applies a basemodel multiple times and take a weighted sum (or max) of the result.\n",
    "    \n",
    "    :parameter Nparallel: The number of times to apply in parallel\n",
    "    :type Nparallel: int\n",
    "    :parameter basemodel: a keras.Model inferred to have Nin inputs and Nout outputs.\n",
    "    :type basemodel: a keras.Model\n",
    "    :parameter insteadmax: If True, take the max of the results of the basemodel instead of the weighted sum.\n",
    "        For compatibility, the model is still constructed with weights as inputs, but it ignores them.\n",
    "    :type insteadmax: Boolean\n",
    "    :returns: model with inputs shape [(?,Nparallel),(?,Nin,Nparallel)] and outputs shape (?,Nout).\n",
    "        The first input is the scalar weights in the sum.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    Note: We could do a max over the parallel applications instead of or in addition to the weighted sum.\n",
    "    \n",
    "    \"\"\"\n",
    "    # infer shape of basemodel inputs and outputs\n",
    "    Nin =  basemodel.inputs[0].shape[1]\n",
    "    Nout =  basemodel.outputs[0].shape[1]\n",
    "    \n",
    "    # Apply basemodel Nparallel times in parallel\n",
    "    # create main input (?,Nparallel,Nin) \n",
    "    parallel_inputs = keras.Input(shape=(Nparallel,Nin), name='parallelwrapper_input0')\n",
    "    # apply base NN to each parallel slice; outputs (?,Nparallel,Nout)\n",
    "    if False:\n",
    "        # original version, stopped working at some tensorflow update\n",
    "        xb = basemodel(parallel_inputs) # worked in earlier tensorflow\n",
    "        #xb = tf.map_fn(basemodel,parallel_inputs) # another version that fails\n",
    "    else:\n",
    "        # newer version, works but makes summary and graphing cumbersome\n",
    "        # unstack in the Nparallel directio\n",
    "        parallel_inputsunstacked = tf.keras.ops.unstack(parallel_inputs, Nparallel, 1)\n",
    "        # apply base NN to each \n",
    "        xbunstacked = [basemodel(x) for x in parallel_inputsunstacked]\n",
    "        # re-stack\n",
    "        xb = tf.keras.ops.stack(xbunstacked,axis=1)\n",
    "    \n",
    "    # create input scalars for weighted sun (?,Nparallel)\n",
    "    weight_inputs = keras.Input(shape=(Nparallel,), name='parallelScalars')\n",
    "    if insteadmax:\n",
    "        # take max over the Nparallel direction to get (?,1,Nout)\n",
    "        out = layers.MaxPool1D(pool_size=Nparallel)(xb)\n",
    "        # reshape to (?,Nout)\n",
    "        out = layers.Reshape((Nout,))(out)\n",
    "    else:\n",
    "        # do a weighted sum over the Nparallel direction to get (?,Nout)\n",
    "        out = layers.Dot((-2,-1))([xb,weight_inputs])\n",
    "    \n",
    "    return keras.Model(inputs=[weight_inputs,parallel_inputs], outputs=out, name='parallelwrapper')\n",
    "if 1:\n",
    "    # manual check\n",
    "    mmd = multiDense(10,4,3)\n",
    "    mmd.summary()\n",
    "    mpw = parallelwrapper(5,mmd,insteadmax=0)\n",
    "    mpw.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_generator(data,labels,baselayers,Nfeatures,endlayers,base_regularizer=None,end_regularizer=None):\n",
    "    \"\"\"Initialize the generator neural net.\n",
    "    \n",
    "    :returns: return generator and descrimina NN.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Option changing how results of each view are aggregated\n",
    "    insteadmax = False # Does weighted average; original design\n",
    "    #insteadmax = True # Does max instead of weighted average (for both G and D)\n",
    "\n",
    "    # G\n",
    "    # base NN\n",
    "    Gbase = multiDense(data[1].shape[2],Nfeatures,baselayers,kernel_regularizer=base_regularizer) \n",
    "    # parallel view wrapper\n",
    "    Gpw = parallelwrapper(Nviews,Gbase,insteadmax)\n",
    "    # features to toxicity\n",
    "    #Gft = multiDense(Nfeatures,labels.shape[1],endlayers,kernel_regularizer=end_regularizer)\n",
    "    #we got an error when running original line since energies are one dimnsional array so we can interpret the single dimension as having only one dimension\n",
    "    Gft = multiDense(Nfeatures,1,endlayers,kernel_regularizer=end_regularizer) \n",
    "    # string together\n",
    "    generator = keras.Model(inputs=Gpw.inputs,outputs=Gft(Gpw.outputs),name='generator')\n",
    "    # make trainable\n",
    "    generator.compile(optimizer='adam',loss='mse')\n",
    "    #generator.summary()\n",
    "    # previously did better with Nfeatures=Ntoxicity and no Gft\n",
    "\n",
    "    if 0:\n",
    "        # sanity checks that model is working\n",
    "        print(\"Sanity check:\")\n",
    "        ws,vs = data\n",
    "        gbv0call = Gbase(vs[:,0,:]).numpy()\n",
    "        gbv0predict = Gbase.predict(vs[:,0,:])\n",
    "        print(\"base: 0 ?==\", np.linalg.norm(gbv0call-gbv0predict))\n",
    "        gpwcall = Gpw([ws,vs]).numpy()\n",
    "        gpwpredict = G\n",
    "        pw.predict([ws,vs])\n",
    "        print(\"wrapper: 0 ?==\",np.linalg.norm(gpwcall-gpwpredict))\n",
    "        gencall = generator([ws,vs]).numpy()\n",
    "        genpredict = generator.predict([ws,vs])\n",
    "        print(\"whole: 0 ?==\",np.linalg.norm(gencall-genpredict))\n",
    "        \n",
    "    return generator\n",
    "baselayers = 2  # hidden layers before weighted sum\n",
    "base_reg = 0.1  # regularization for the base layers\n",
    "Nfeatures = 1  # number of outputs of weighted sum\n",
    "endlayers = 1  # hidden layers after weighted sum\n",
    "end_reg = 0.1  # regularization for the end layers\n",
    "\n",
    "if base_reg == 0:\n",
    "    base_regularizer = None\n",
    "else:\n",
    "    base_regularizer = regularizers.l2(base_reg)\n",
    "\n",
    "if end_reg == 0:\n",
    "    end_regularizer = None\n",
    "else:\n",
    "    end_regularizer = regularizers.l2(end_reg)  #???\n",
    "\n",
    "print(\"(baselayers, base_reg, Nfeatures, endlayers, end_reg) =\",\n",
    "      (baselayers, base_reg, Nfeatures, endlayers, end_reg))\n",
    "# compile model with options\n",
    "generator = init_generator(dataG_train,labelsG_train,baselayers,Nfeatures,endlayers,\n",
    "                           base_regularizer=base_regularizer,end_regularizer=end_regularizer)\n",
    "generator.compile(optimizer='adam',loss='mse')\n",
    "#generator.summary()\n",
    "#change loss to MAE for energies\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit\n",
    "BATCH_SIZE = 32\n",
    "if 1: \n",
    "    history = generator.fit(dataG_train,labelsG_train,\n",
    "                  epochs=200,batch_size=BATCH_SIZE,verbose=0,\n",
    "                  validation_data=(dataG_test, labelsG_test))\n",
    "    print(\"train loss=\",generator.evaluate(dataG_train,labelsG_train,verbose=0))\n",
    "    print(\"test loss=\",generator.evaluate(dataG_test,labelsG_test,verbose=0))\n",
    "#     generator.fit(dataG_train,labelsG_train,epochs=1000,batch_size=BATCH_SIZE,verbose=0)\n",
    "#    generator.save(modelpath+\"AG-model-RT.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions of G versus truth\n",
    "def PvT_plot(model,data,labels,legend=None,title=None,doresidual=False):\n",
    "    \"\"\"Construct a plot of the true labels (x-axis) vs the data generated by the model (y-axis).\n",
    "    \n",
    "    :parameter model: the model (e.g. NN)\n",
    "    :type model: keras.model\n",
    "    :parameter data: the data that can be input to the model\n",
    "    :type data: numpy.array\n",
    "    :parameter labels: the true outputs corresponding to the data\n",
    "    :type labels: numpy.array\n",
    "    :parameter legend: The string labels corresponding to the columns of labels \n",
    "    :type legend: None or list of str\n",
    "    :parameter title: A title for the plot\n",
    "    :type title: None or string\n",
    "    :parameter doresidual: If true, plot the residual instead\n",
    "    :type doresidual: Boolean\n",
    "    \"\"\"\n",
    "        \n",
    "    gen_lab = model.predict(data)\n",
    "    if doresidual:\n",
    "        gen_lab = gen_lab - labels\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)        \n",
    "    if legend is None:\n",
    "        for i in range(labels.shape[1]):\n",
    "            plt.plot(labels[:,i],gen_lab[:,i],'o')\n",
    "    else:\n",
    "        for i in range(labels.shape[1]):\n",
    "            # include legend\n",
    "            plt.plot(labels[:,i],gen_lab[:,i],'o', label=legend[i])\n",
    "        ax.legend(bbox_to_anchor=(1.04,1), borderaxespad=0)\n",
    "    ax.set_xlabel('True Values')\n",
    "    if doresidual:\n",
    "        ax.set_ylabel('Residual Values')\n",
    "    else:\n",
    "        ax.set_ylabel('Generated Values')\n",
    "        # reference line\n",
    "        mintrue = np.min(labels)\n",
    "        maxtrue = np.max(labels)\n",
    "        plt.plot([mintrue,maxtrue],[mintrue,maxtrue])\n",
    "    if title is None:\n",
    "        title = ''\n",
    "    if 1:\n",
    "        if not doresidual:\n",
    "            gen_lab = gen_lab - labels\n",
    "        MSE = np.square(gen_lab).mean()\n",
    "        title = title+\" Mean Squared Error=\"+str(MSE)\n",
    "        print(\"Mean Squared Error: \", MSE)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "if 1:\n",
    "    plt.figure()\n",
    "    if base_regularizer is None and end_regularizer is None:\n",
    "        plt.title('Loss (no regularization)')\n",
    "    else:\n",
    "        plt.title(f'Loss, including regularization (base_reg,end_reg)=({base_reg},{end_reg})')\n",
    "    plt.semilogy(history.history['loss'], label='train')\n",
    "    plt.semilogy(history.history['val_loss'], label='test')\n",
    "    plt.ylim((0, plt.ylim()[1]))  # Set lower ylim to 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
