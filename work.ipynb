{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# standard python\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "#import pathlib\n",
    "\n",
    "import os\n",
    "# plotting, especially for jupyter notebooks\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True # breaks for some endpoint labels\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() # needed for tf version 1 or it stages operations but does not do them\n",
    "from tensorflow import keras\n",
<<<<<<< HEAD
    "from keras import layers, models, optimizers, losses, metrics\n",
=======
>>>>>>> origin/main
    "from tensorflow.keras import layers, regularizers\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "# local routines\n",
    "#from chemdataprep import load_PDBs,load_countsfromPDB,load_diametersfromPDB,find_chemnames\n",
<<<<<<< HEAD
    "#from toxmathandler import load_tscores\n",
=======
    "from toxmathandler import load_tscores\n",
>>>>>>> origin/main
    "\n",
    "#checkpoint_path = \"/home2/ajgreen4/Read-Across_w_GAN/Models/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__,\". Executing eagerly?\",tf.executing_eagerly())\n",
    "print(\"Number of GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weightedviews\n",
<<<<<<< HEAD
    "#import toxmathandler\n",
=======
    "import toxmathandler\n",
>>>>>>> origin/main
    "import qm9datapreparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qm9datapreparation import all_molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_molecules))\n",
<<<<<<< HEAD
    "print(all_molecules[40000])\n",
    "len(all_molecules)\n"
=======
    "print(all_molecules[133726])"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_atoms = set()\n",
    "\n",
    "for molecule in all_molecules:\n",
    "    for atom in molecule:\n",
    "        unique_atoms.add(atom[0])\n",
    "unique_atoms = sorted(unique_atoms)\n",
    "print(\"atom types:\", unique_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightedviews import load_qm9_data\n",
    "from weightedviews import speciesmap\n",
    "ws, vs, Natoms, Nviews = load_qm9_data(all_molecules, speciesmap, setNatoms= None, setNviews= None, carbonbased= False, verbose= 1)"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ese import elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ese_element = np.array(elements)\n",
    "ese_element.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_molecules_train, all_molecules_test, ese_element_train, ese_element_test = train_test_split(all_molecules, ese_element, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_molecules_test))\n",
    "print(len(all_molecules_train))\n",
    "print(len(ese_element_test))\n",
    "print(len(ese_element_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_train = all_molecules_train\n",
    "qm9_test = all_molecules_test\n",
    "qm9_labels_train = ese_element_train\n",
    "qm9_labels_test= ese_element_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_qm9_train, vs_qm9_train, Natoms_train, Nviews_train = load_qm9_data(qm9_train, speciesmap, setNatoms= None, setNviews= None, carbonbased= False, verbose= 1)\n",
    "qm9G_train = [ws_qm9_train, vs_qm9_train]\n",
    "ws_qm9_test, vs_qm9_test, Natoms_test, Nviews_test = load_qm9_data(qm9_test, speciesmap, setNatoms= None, setNviews= None, carbonbased= False, verbose= 1)\n",
    "qm9G_test = [ws_qm9_test, vs_qm9_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsG_train = qm9_labels_train\n",
    "labelsG_test = qm9_labels_test\n",
    "Ntoxicity =1\n",
    "ws_train, vs_train = ws_qm9_train, vs_qm9_train\n",
    "ws_test, vs_test = ws_qm9_test, vs_qm9_test\n",
    "dataG_train = [ws_train, vs_train]\n",
    "dataG_test = [ws_test, vs_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network code\n",
    "Constructor routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic dense NN\n",
    "def multiDense(Nin,Nout,Nhidden,widthhidden=None,kernel_regularizer=None):\n",
    "    \"\"\"Construct a basic NN with some dense layers.\n",
    "    \n",
    "    :parameter Nin: The number of inputs\n",
    "    :type Nin: int\n",
    "    :parameter Nout: The number of outputs\n",
    "    :type Nout: int\n",
    "    :parameter Nhidden: The number of hidden layers.\n",
    "    :type Nhidden: int\n",
    "    :parameter widthhidden: The width of each hidden layer.\n",
    "        If left at None, Nin + Nout will be used.\n",
    "    :parameter kernel_regularizer: the regularizer to use, such as regularizers.l2(0.001)\n",
    "    :type kernel_regularizer: tensorflow.keras.regularizers.xxx\n",
    "    :returns: The NN model\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    if widthhidden is None:\n",
    "        widthhidden = Nin + Nout\n",
    "    x = inputs = keras.Input(shape=(Nin,), name='multiDense_input')\n",
    "    if kernel_regularizer is not None:\n",
    "        print(\"Using regularization\")\n",
    "    for i in range(Nhidden):\n",
    "        x = layers.Dense(widthhidden, activation='relu', kernel_regularizer=kernel_regularizer,name='dense'+str(i))(x)\n",
    "#        x = layers.Dense(widthhidden, name='dense'+str(i))(x)\n",
    "#        x = tf.nn.leaky_relu(x, alpha=0.05)\n",
    "#    outputs = layers.Dense(Nout, activation='linear',name='multiDense_output')(x)\n",
    "    outputs = layers.Dense(Nout,name='multiDense_output')(x)\n",
    "    #outputs = tf.nn.leaky_relu(outputs, alpha=0.05)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)#, name='multiDense')\n",
    "if 1:\n",
    "    # manual check of multiDense\n",
    "    mmd = multiDense(10,4,3)\n",
    "    mmd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parallelwrapper(Nparallel, basemodel, insteadmax=False):\n",
    "    \"\"\"Construct a model that applies a basemodel multiple times and take a weighted sum (or max) of the result.\n",
    "    \n",
    "    :parameter Nparallel: The number of times to apply in parallel\n",
    "    :type Nparallel: int\n",
    "    :parameter basemodel: a keras.Model inferred to have Nin inputs and Nout outputs.\n",
    "    :type basemodel: a keras.Model\n",
    "    :parameter insteadmax: If True, take the max of the results of the basemodel instead of the weighted sum.\n",
    "        For compatibility, the model is still constructed with weights as inputs, but it ignores them.\n",
    "    :type insteadmax: Boolean\n",
    "    :returns: model with inputs shape [(?,Nparallel),(?,Nin,Nparallel)] and outputs shape (?,Nout).\n",
    "        The first input is the scalar weights in the sum.\n",
    "    :rtype: keras.Model\n",
    "    \"\"\"\n",
    "    # Infer shape of basemodel inputs and outputs\n",
    "    Nin = int(basemodel.inputs[0].shape[1])\n",
    "    Nout = int(basemodel.outputs[0].shape[1])\n",
    "    \n",
    "    # Apply basemodel Nparallel times in parallel\n",
    "    # Create main input (?, Nparallel, Nin)\n",
    "    parallel_inputs = keras.Input(shape=(Nparallel, Nin), name='parallelwrapper_input0')\n",
    "    print(f\"parallel_inputs.shape: {parallel_inputs.shape}\")\n",
    "\n",
    "    # Split the inputs\n",
    "    parallel_inputssplit = tf.split(parallel_inputs, num_or_size_splits=Nparallel, axis=1)\n",
    "    print(f\"parallel_inputssplit: {[x.shape for x in parallel_inputssplit]}\")\n",
    "\n",
    "    # Reshape to remove the extra dimension added by split\n",
    "    parallel_inputssplit = [tf.squeeze(x, axis=1) for x in parallel_inputssplit]\n",
    "    print(f\"parallel_inputssplit after squeeze: {[x.shape for x in parallel_inputssplit]}\")\n",
    "\n",
    "    # Apply base NN to each\n",
    "    xbunstacked = [basemodel(x) for x in parallel_inputssplit]\n",
    "    print(f\"xbunstacked: {[x.shape for x in xbunstacked]}\")\n",
    "\n",
    "    # Stack the results\n",
    "    xb = tf.stack(xbunstacked, axis=1)\n",
    "    print(f\"xb.shape after stacking: {xb.shape}\")\n",
    "\n",
    "    # Create input scalars for weighted sum (?, Nparallel)\n",
    "    weight_inputs = keras.Input(shape=(Nparallel,), name='parallelScalars')\n",
    "\n",
    "    if insteadmax:\n",
    "        # Take max over the Nparallel direction to get (?, 1, Nout)\n",
    "        out = layers.MaxPool1D(pool_size=Nparallel)(xb)\n",
    "        # Reshape to (?, Nout)\n",
    "        out = layers.Reshape((Nout,))(out)\n",
    "    else:\n",
    "        # Do a weighted sum over the Nparallel direction to get (?, Nout)\n",
    "        out = layers.Dot(axes=(-2, -1))([xb, weight_inputs])\n",
    "\n",
    "    return keras.Model(inputs=[weight_inputs, parallel_inputs], outputs=out, name='parallelwrapper')\n",
    "\n",
    "# Function to create a simple dense model for testing\n",
    "def multiDense(Nin, Nout, Nhidden, widthhidden=None, kernel_regularizer=None):\n",
    "    if widthhidden is None:\n",
    "        widthhidden = Nin + Nout\n",
    "    x = inputs = keras.Input(shape=(Nin,), name='multiDense_input')\n",
    "    for i in range(Nhidden):\n",
    "        x = layers.Dense(widthhidden, activation='relu', kernel_regularizer=kernel_regularizer, name='dense'+str(i))(x)\n",
    "    outputs = layers.Dense(Nout, name='multiDense_output')(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Manual check\n",
    "mmd = multiDense(10, 4, 3)\n",
    "mmd.summary()\n",
    "mpw = parallelwrapper(5, mmd, insteadmax=0)\n",
    "mpw.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
=======
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
>>>>>>> origin/main
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
